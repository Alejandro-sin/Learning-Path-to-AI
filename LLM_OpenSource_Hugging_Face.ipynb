{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alejandro-sin/Learning-Path-to-AI/blob/master/LLM_OpenSource_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHEUf_Xe98Di"
      },
      "source": [
        "# Hugging Faces\n",
        "\n",
        "\n",
        "Hugging Face es una plataforma líder en inteligencia artificial de código abierto. Ofrece un repositorio de modelos, conjuntos de datos y espacios dedicados a la inferencia. Este notebook explora cómo utilizar algunas de las herramientas y modelos disponibles en Hugging Face para diversas tareas de procesamiento de lenguaje natural y generación de contenido multimedia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wCpTxkBUzxsO"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets diffusers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4DMB6Xp3ozA"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e3f3N4zb1Ph9"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "\n",
        "\n",
        "## AUTH\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "# TORCH - Diffsuser\n",
        "import torch\n",
        "from diffusers import FluxPipeline\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Display Image and Sound\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "from PIL import Image\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KL49qHao51Rs"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPFQqvGs3zZz"
      },
      "source": [
        "## Pipelines\n",
        "\n",
        "\n",
        "Los pipelines en HugginFace proveen una interfaz de alto nivel para operar con los modelos, y realizar inferencias con los modelos ya entrenados. La idea es revisar algunas de las tareas que podemos hacer con los transformers.\n",
        "\n",
        "- Analisis de Sentimientos\n",
        "- Named Entity Recognition\n",
        "- Preguntas y respuestas con contexto\n",
        "- Resumen de texto\n",
        "- Traducción y clasificaión\n",
        "- Generación de imagenes, audio y video.\n",
        "- Feature Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhODu2hthp8P"
      },
      "source": [
        "### Anlisis de Sentimiento.\n",
        "\n",
        "\n",
        "Utilizamos un pipeline de análisis de sentimiento para evaluar el tono de un texto. Este pipeline utiliza un modelo preentrenado para clasificar el texto como positivo o negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIBAAJtkg0H5",
        "outputId": "0368a7f5-937c-4738-8599-f97b633dddb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "result = classifier(\"\"\"Rainer Maria Rilke is considered one of the most lyrically intense German-language poets.\n",
        "            His work spans the late 19th and early 20th centuries, bridging the gap between the traditional Romantic era and the rising tide of Modernism.\"\"\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyZp6OvBkKpB"
      },
      "source": [
        "\n",
        "### Extracción de Entidades Nombradas (NER)\n",
        "\n",
        "La extracción de entidades nombradas es una tarea común en el procesamiento de lenguaje natural. Aquí, etiquetamos el texto con diferentes categorías como personas, organizaciones y lugares. También mostramos cómo utilizar un modelo especializado para contextos médicos.\n",
        "\n",
        "\n",
        "- Si no pasamos un modelo en concreto lo que sucederá es que se usará uno por defecto, en este caso se usó el modelo por defecto para texto en inglés `dbmdz/bert-large-cased-finetuned-conll03-english`\n",
        "\n",
        "- Podemos usar ejemplos más especializados, por ejemplo para contextos medicos donde el modelo se finetuneo para reconocer estas entidades. `blaze999/Medical-NER`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5SglaW8kJfJ"
      },
      "outputs": [],
      "source": [
        "ner = pipeline(\"ner\", grouped_entities=True, device=\"cuda\")\n",
        "result = ner(\"\"\"Barack Obama was the 44th president of the United States\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef3XQEWDkJc6"
      },
      "outputs": [],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdY51fL7kJaB"
      },
      "outputs": [],
      "source": [
        "ner = pipeline(\"token-classification\", model=\"blaze999/Medical-NER\")\n",
        "result = ner(\"\"\"A 48 year-old female presented with vaginal bleeding and abnormal Pap smears.\n",
        "Upon diagnosis of invasive non-keratinizing SCC of the cervix, she underwent a radical hysterectomy with salpingo-oophorectomy\n",
        "which demonstrated positive spread to the pelvic lymph nodes and the parametrium.\n",
        "Pathological examination revealed that the tumour also extensively involved the lower uterine segment.\"\"\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6EkLYX4l103"
      },
      "source": [
        "### Preguntas y Respuestas\n",
        "\n",
        "Este pipeline permite responder preguntas basadas en un contexto proporcionado. Es útil para tareas de comprensión de lectura y búsqueda de información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFvH0nwskJXa"
      },
      "outputs": [],
      "source": [
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=\"Who was the 44th president of the United States?\", context=\"Barack Obama was the 44th president of the United States.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tomwpKFVl86U"
      },
      "source": [
        "### Resumenes\n",
        "\n",
        "Generamos resúmenes de texto utilizando un pipeline de resumen. Esto es útil para condensar información extensa en un formato más manejable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEAYP46fl_Kq"
      },
      "outputs": [],
      "source": [
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"A knowledge graph, also known as a semantic network, represents a network of real-world entities—such as objects, events,\n",
        "situations or concepts—and illustrates the relationship between them.\n",
        "This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge “graph.”\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-de0E3FqmVgQ"
      },
      "source": [
        "Podemos usar generadores también para producir los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l9KkaYbmU-0"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"A knowledge graph, also known as a semantic network, represents a network of real-world entities—such as objects, events,\n",
        "    situations or concepts—and illustrates the relationship between them.\n",
        "    This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge “graph.”\n",
        "\"\"\"\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "\n",
        "def pipeline_generator(text):\n",
        "  '''\n",
        "  Función sencilla para resumir texto.\n",
        "\n",
        "  '''\n",
        "  sentinel = 0\n",
        "  while True:\n",
        "    summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "    if sentinel == 1:\n",
        "      break\n",
        "    sentinel += 1\n",
        "    yield summary[0]['summary_text']\n",
        "\n",
        "for n in pipeline_generator(text):\n",
        "  print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d3PIgv9n3A6"
      },
      "source": [
        "### Traducción\n",
        "\n",
        "Utilizamos modelos de traducción para convertir texto de un idioma a otro. Aquí mostramos ejemplos de traducción de inglés a francés y español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T3zJ2g1ml9K"
      },
      "outputs": [],
      "source": [
        "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNcEtqdbml6e"
      },
      "outputs": [],
      "source": [
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clasificación de Texto\n",
        "\n",
        "La clasificación de texto permite categorizar texto en diferentes etiquetas. Utilizamos un pipeline de clasificación sin entrenamiento previo para clasificar un texto en categorías como tecnología, deportes y política."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "066JUGu3n-uG"
      },
      "outputs": [],
      "source": [
        "# Reutilicemos el texto de los grafos de conocimiento para clasificar.\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(text, candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGqEpvff0dCb"
      },
      "source": [
        "### Generación de Imágenes\n",
        "\n",
        "Exploramos la generación de imágenes a partir de texto utilizando modelos de difusión. Estos modelos pueden crear imágenes en estilos artísticos específicos basados en descripciones textuales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZvaYAbZ5dM4"
      },
      "outputs": [],
      "source": [
        "if 1==1:\n",
        "  image_gen = DiffusionPipeline.from_pretrained(\n",
        "      \"stabilityai/stable-diffusion-2\",\n",
        "      torch_dtype=torch.float16,\n",
        "      use_safetensors=True,\n",
        "      variant=\"fp16\"\n",
        "      ).to(\"cuda\")\n",
        "\n",
        "  #text = \"A reunion of Genius and Maldoror talking each other, in the surreal style of Salvador Dali\"\n",
        "  text = \"A lonely person flying and dreaming, in the style of Impresionism of Monet\"\n",
        "  image = image_gen(prompt=text).images[0]\n",
        "  display(image)\n",
        "  # image.save(\"surreal.png\")\n",
        "\n",
        "\n",
        "# Requieres more GPU like A100\n",
        "if 1==0:\n",
        "  pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
        "  generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
        "  prompt = \"A futuristic class full of students learning AI coding in the surreal style of Salvador Dali\"\n",
        "\n",
        "  # Generate the image using the GPU\n",
        "  image = pipe(\n",
        "      prompt,\n",
        "      guidance_scale=0.0,\n",
        "      num_inference_steps=4,\n",
        "      max_sequence_length=256,\n",
        "      generator=generator\n",
        "  ).images[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tsu6_8PjvbD"
      },
      "outputs": [],
      "source": [
        "image.save(\"lonely_person.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raDuDM752b3y"
      },
      "source": [
        "### Generación de Audio\n",
        "\n",
        "Generamos audio a partir de texto utilizando un modelo de síntesis de voz. Este pipeline convierte texto en habla, permitiendo la creación de contenido auditivo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwpCwoRE2eMc"
      },
      "outputs": [],
      "source": [
        "if 1==0:\n",
        "  synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "  embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "  speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "  speech = synthesiser(\"We cant fail if we learn from it\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "  sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
        "  Audio(\"speech.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEEtsTDdjG4R"
      },
      "source": [
        "### Generación de Video\n",
        "\n",
        "Finalmente, exploramos la generación de video a partir de imágenes. Este proceso requiere más recursos de hardware y es un área emergente en la generación de contenido multimedia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzf0BfOOjFr-"
      },
      "outputs": [],
      "source": [
        "if 1==0:\n",
        "  # Esto requiere de mucha más RAM pra ejecutarse\n",
        "  # Cargar una imagen de ejemplo\n",
        "  image_path = \"/content/lonely_person.png\"\n",
        "  image = Image.open(image_path)\n",
        "\n",
        "  pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-video-diffusion-img2vid-xt\")\n",
        "  # Ejecutar la generación de video con la imagen de entrada\n",
        "  output = pipe(image=image)\n",
        "\n",
        "  # Guardar el primer frame del video generado\n",
        "  output.images[0].save(\"output_frame.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tDcwRLBojSs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM4Mnc9rvHiQ5iUaalTtCcZ",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
