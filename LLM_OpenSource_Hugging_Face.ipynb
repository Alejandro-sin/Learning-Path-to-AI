{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM4Mnc9rvHiQ5iUaalTtCcZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alejandro-sin/Learning-Path-to-AI/blob/master/LLM_OpenSource_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Faces\n",
        "\n",
        "\n",
        "Huggin Faces es el sitio por excelencia para la inteligencia artificial opensource, es un espacio que sirve de repositorio de modelos, datasets, y espacios dedicados a la inferencia.\n",
        "\n"
      ],
      "metadata": {
        "id": "gHEUf_Xe98Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets diffusers"
      ],
      "metadata": {
        "id": "wCpTxkBUzxsO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "X4DMB6Xp3ozA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "\n",
        "\n",
        "## AUTH\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "# TORCH - Diffsuser\n",
        "import torch\n",
        "from diffusers import FluxPipeline\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Display Image and Sound\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "from PIL import Image\n",
        "\n"
      ],
      "metadata": {
        "id": "e3f3N4zb1Ph9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "KL49qHao51Rs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines\n",
        "\n",
        "\n",
        "Los pipelines en HugginFace proveen una interfaz de alto nivel para operar con los modelos, y realizar inferencias con los modelos ya entrenados. La idea es revisar algunas de las tareas que podemos hacer con los transformers.\n",
        "\n",
        "- Analisis de Sentimientos\n",
        "- Named Entity Recognition\n",
        "- Preguntas y respuestas con contexto\n",
        "- Resumen de texto\n",
        "- Traducción y clasificaión\n",
        "- Generación de imagenes, audio y video.\n",
        "- Feature Extraction\n"
      ],
      "metadata": {
        "id": "UPFQqvGs3zZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Anlisis de Sentimiento."
      ],
      "metadata": {
        "id": "EhODu2hthp8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "result = classifier(\"\"\"Rainer Maria Rilke is considered one of the most lyrically intense German-language poets.\n",
        "            His work spans the late 19th and early 20th centuries, bridging the gap between the traditional Romantic era and the rising tide of Modernism.\"\"\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIBAAJtkg0H5",
        "outputId": "0368a7f5-937c-4738-8599-f97b633dddb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER\n",
        "\n",
        "La extracción de entidades es una tarea muy común en las operaciones de lenguaje natural, buscamos etiquetar textos condiferentes TAGs asociados a la entidad. Persoans, organizaciones, lugares y demás.\n",
        "\n",
        "- Si no pasamos un modelo en concreto lo que sucederá es que se usará uno por defecto, en este caso se usó el modelo por defecto para texto en inglés `dbmdz/bert-large-cased-finetuned-conll03-english`\n",
        "\n",
        "- Podemos usar ejemplos más especializados, por ejemplo para contextos medicos donde el modelo se finetuneo para reconocer estas entidades. `blaze999/Medical-NER`"
      ],
      "metadata": {
        "id": "pyZp6OvBkKpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\", grouped_entities=True, device=\"cuda\")\n",
        "result = ner(\"\"\"Barack Obama was the 44th president of the United States\"\"\")\n"
      ],
      "metadata": {
        "id": "L5SglaW8kJfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "id": "Ef3XQEWDkJc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"token-classification\", model=\"blaze999/Medical-NER\")\n",
        "result = ner(\"\"\"A 48 year-old female presented with vaginal bleeding and abnormal Pap smears.\n",
        "Upon diagnosis of invasive non-keratinizing SCC of the cervix, she underwent a radical hysterectomy with salpingo-oophorectomy\n",
        "which demonstrated positive spread to the pelvic lymph nodes and the parametrium.\n",
        "Pathological examination revealed that the tumour also extensively involved the lower uterine segment.\"\"\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "rdY51fL7kJaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preguntas y Respuestas"
      ],
      "metadata": {
        "id": "Q6EkLYX4l103"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=\"Who was the 44th president of the United States?\", context=\"Barack Obama was the 44th president of the United States.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "KFvH0nwskJXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resumenes"
      ],
      "metadata": {
        "id": "tomwpKFVl86U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"A knowledge graph, also known as a semantic network, represents a network of real-world entities—such as objects, events,\n",
        "situations or concepts—and illustrates the relationship between them.\n",
        "This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge “graph.”\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "rEAYP46fl_Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos usar generadores también para producir los resultados"
      ],
      "metadata": {
        "id": "-de0E3FqmVgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"A knowledge graph, also known as a semantic network, represents a network of real-world entities—such as objects, events,\n",
        "    situations or concepts—and illustrates the relationship between them.\n",
        "    This information is usually stored in a graph database and visualized as a graph structure, prompting the term knowledge “graph.”\n",
        "\"\"\"\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "\n",
        "def pipeline_generator(text):\n",
        "  '''\n",
        "  Función sencilla para resumir texto.\n",
        "\n",
        "  '''\n",
        "  sentinel = 0\n",
        "  while True:\n",
        "    summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "    if sentinel == 1:\n",
        "      break\n",
        "    sentinel += 1\n",
        "    yield summary[0]['summary_text']\n",
        "\n",
        "for n in pipeline_generator(text):\n",
        "  print(n)"
      ],
      "metadata": {
        "id": "9l9KkaYbmU-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traducción"
      ],
      "metadata": {
        "id": "8d3PIgv9n3A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ],
      "metadata": {
        "id": "3T3zJ2g1ml9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All translation models are here: https://huggingface.co/models?pipeline_tag=translation&sort=trending\n",
        "\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The Data Scientists were truly amazed by the power and simplicity of the HuggingFace pipeline API.\")\n",
        "print(result[0]['translation_text'])"
      ],
      "metadata": {
        "id": "aNcEtqdbml6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reutilicemos el texto de los grafos de conocimiento para clasificar.\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(text, candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "066JUGu3n-uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de Imagenes\n"
      ],
      "metadata": {
        "id": "TGqEpvff0dCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 1==1:\n",
        "  image_gen = DiffusionPipeline.from_pretrained(\n",
        "      \"stabilityai/stable-diffusion-2\",\n",
        "      torch_dtype=torch.float16,\n",
        "      use_safetensors=True,\n",
        "      variant=\"fp16\"\n",
        "      ).to(\"cuda\")\n",
        "\n",
        "  #text = \"A reunion of Genius and Maldoror talking each other, in the surreal style of Salvador Dali\"\n",
        "  text = \"A lonely person flying and dreaming, in the style of Impresionism of Monet\"\n",
        "  image = image_gen(prompt=text).images[0]\n",
        "  display(image)\n",
        "  # image.save(\"surreal.png\")\n",
        "\n",
        "\n",
        "# Requieres more GPU like A100\n",
        "if 1==0:\n",
        "  pipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
        "  generator = torch.Generator(device=\"cuda\").manual_seed(0)\n",
        "  prompt = \"A futuristic class full of students learning AI coding in the surreal style of Salvador Dali\"\n",
        "\n",
        "  # Generate the image using the GPU\n",
        "  image = pipe(\n",
        "      prompt,\n",
        "      guidance_scale=0.0,\n",
        "      num_inference_steps=4,\n",
        "      max_sequence_length=256,\n",
        "      generator=generator\n",
        "  ).images[0]\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "nZvaYAbZ5dM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.save(\"lonely_person.png\")"
      ],
      "metadata": {
        "id": "4tsu6_8PjvbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de audio\n",
        "\n"
      ],
      "metadata": {
        "id": "raDuDM752b3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 1==0:\n",
        "  synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "  embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "  speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "  speech = synthesiser(\"We cant fail if we learn from it\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "  sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
        "  Audio(\"speech.wav\")"
      ],
      "metadata": {
        "id": "GwpCwoRE2eMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de Video\n"
      ],
      "metadata": {
        "id": "zEEtsTDdjG4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 1==0:\n",
        "  # Esto requiere de mucha más RAM pra ejecutarse\n",
        "  # Cargar una imagen de ejemplo\n",
        "  image_path = \"/content/lonely_person.png\"\n",
        "  image = Image.open(image_path)\n",
        "\n",
        "  pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-video-diffusion-img2vid-xt\")\n",
        "  # Ejecutar la generación de video con la imagen de entrada\n",
        "  output = pipe(image=image)\n",
        "\n",
        "  # Guardar el primer frame del video generado\n",
        "  output.images[0].save(\"output_frame.png\")"
      ],
      "metadata": {
        "id": "xzf0BfOOjFr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0tDcwRLBojSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}